---
layout: post
title: Finding OLS Estimators
tags: [statistics, regression]
---

I'm beginning a new school year and I'm currently taking a Regression class. It's honestly a review of a lot of stuff I've seen before and one thing we've covered is the OLS method of estimation which is near and dear to many of our hearts. However, it's been years (okay more like 3) since I've actually dervied it and I struggled initially. It required a few "tricks" or knowledge of how to rearrange or replace terms with other terms. So I decided to write it here in hopes I can remember it better.

\begin{enumerate}
    \item Take 1st partial derivatives of SSE wrt to $\beta_0$ and $\beta_1$ and set to 0 (I use hats and no hats interchangeably).
    $$\frac{\partial SSE}{\partial \beta_0} = -2\sum_{i=1}^n(Y_i - \beta_0 - \beta_1X_i) = 0.$$
    $$\frac{\partial SSE}{\partial \beta_1} = -2\sum_{i=1}^nX_i(Y_i - \beta_0 - \beta_1X_i) = 0.$$
    \item Solve for $\beta_0$ and $\beta_1$:
    $$\sum(y_i - \hat\beta_0 - \hat \beta_1x_i) = 0$$
    $$n\beta_0 =\sum y_i - \beta_1 \sum x_i$$
    $$\beta_0 =\bar y - \beta_1 \bar x \;\; [\bar . = \frac{1}{n}\sum ._i]$$\\
    
    Now for $\beta_1:$
    $$\sum x_iy_i - \beta_0x_i - \beta_1x_i^2 = 0$$
    $$\sum (x_iy_i - (\bar y - \beta_1 \bar x)x_i - \beta_1x_i^2)= 0$$
    $$\sum x_iy_i - \bar y\sum x_i + \beta_1 \bar x\sum x_i - \beta_1 \sum x_i^2= 0$$
    $$\sum x_iy_i - \bar y\sum x_i = \beta_1 (\bar x\sum x_i  \sum x_i^2)$$
    $$\frac{\sum x_iy_i - \bar y\sum x_i}{\bar x\sum x_i - \sum x_i^2} = \beta_1 $$
    $$\frac{\sum x_iy_i - n \bar y\bar x}{n\bar x^2 - \sum x_i^2} = \beta_1 $$
    
    Note that:
    $$\sum (x_i - \bar x)(y_i - \bar y) = \sum x_iy_i - n\bar x \bar y \;\;\;\; \sum(x_i - \bar x)^2 = \sum x_i^2 - n\bar x^2.$$
    
    So then: $$\hat \beta_1 = \frac{\sum_{i=1}^n (X_i - \bar X) (Y_i - \bar Y)}{\sum_{i=1}^n (X_i - \bar  X)^2}$$
\end{enumerate}
